# Project Purpose

This is an LLM benchmarking system that tests AI models across multiple coding challenges to compare their performance. The system automatically generates implementations from different AI models and evaluates them on:

## Benchmark Types

1. **Performance Benchmarks** ‚ö°Ô∏è - Raw speed & memory efficiency tests:
   - CSV data processing at scale
   - Graph shortest path algorithms  
   - LRU cache implementations
   - Run-length encoding optimization

2. **Program Fixer Benchmarks** üõ†Ô∏è - AI debugging & code repair challenges:
   - Calendar system debugging
   - Parking garage logic repair
   - School library management fixes
   - Vending machine state handling

## Key Features

- Automated implementation generation using OpenRouter API models
- Fair comparison with consistent prompts across models
- Comprehensive metrics including speed, memory, test success rates, and code quality
- Interactive React website for exploring results
- Transparent, open-source approach with public results